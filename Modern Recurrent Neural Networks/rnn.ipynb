{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ecb66b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025e9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import collections\n",
    "import math\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e6fae",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d43cac",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae04e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "text = \"\"\n",
    "full_text = \"\"\n",
    "with open('The Time Machine - Sample.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "    full_text = preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d4f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "    \n",
    "def tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "def build(raw_text, vocab=None):\n",
    "    tokens = tokenize(preprocess(raw_text))\n",
    "    if vocab is None: vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for token in tokens]\n",
    "    return corpus, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c77930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageData(nn.Module):\n",
    "    def _download(self):\n",
    "        fname = \"The Time Machine - Sample.txt\"\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        return list(text)\n",
    "\n",
    "    def build(self, raw_text, vocab=None):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        tokens = self._tokenize(self._preprocess(raw_text))\n",
    "        if vocab is None: vocab = Vocab(tokens)\n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "        \"\"\"Defined in :numref:`sec_language-model`\"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        corpus, self.vocab = self.build(self._download())\n",
    "        array = torch.tensor([corpus[i:i+num_steps+1]\n",
    "                            for i in range(len(corpus)-num_steps)])\n",
    "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"Defined in :numref:`subsec_partitioning-seqs`\"\"\"\n",
    "        idx = slice(0, self.num_train) if train else slice(\n",
    "            self.num_train, self.num_train + self.num_val)\n",
    "        return self.get_tensorloader([self.X, self.Y], train, idx)\n",
    "    \n",
    "    def get_tensorloader(self, tensors, train, idx_slice):\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            tensors[0][idx_slice], tensors[1][idx_slice]\n",
    "        )\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=train\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61fe75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(nn.Module):\n",
    "    \"\"\"The RNN model implemented from scratch.\n",
    "\n",
    "    Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "        self.W_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.W_hh = nn.Parameter(\n",
    "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        if state is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                              device=inputs.device)\n",
    "        else:\n",
    "            state, = state\n",
    "        outputs = []\n",
    "        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
    "            state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                             torch.matmul(state, self.W_hh) + self.b_h)\n",
    "            outputs.append(state)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72caae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLMScratch(nn.Module): \n",
    "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lr = lr\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=False)\n",
    "    \n",
    "    def one_hot(self, X):\n",
    "        # Output shape: (num_steps, batch_size, vocab_size)\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
    "        return torch.stack(outputs, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clip_gradients(self, grad_clip_val, model):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "        if norm > grad_clip_val:\n",
    "            for param in params:\n",
    "                param.grad[:] *= grad_clip_val / norm\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        embs = self.one_hot(X)\n",
    "        rnn_outputs, _ = self.rnn(embs, state)\n",
    "        return self.output_layer(rnn_outputs)\n",
    "\n",
    "    def predict(self, prefix, num_preds, vocab, device=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        state, outputs = None, [vocab[prefix[0]]]\n",
    "        for i in range(len(prefix) + num_preds - 1):\n",
    "            X = torch.tensor([[outputs[-1]]], device=device)\n",
    "            embs = self.one_hot(X)\n",
    "            rnn_outputs, state = self.rnn(embs, state)\n",
    "            if i < len(prefix) - 1:  # Warm-up period\n",
    "                outputs.append(vocab[prefix[i + 1]])\n",
    "            else:  # Predict num_preds steps\n",
    "                Y = self.output_layer(rnn_outputs)\n",
    "                outputs.append(int(torch.reshape(torch.argmax(Y, axis=2), (1,))))\n",
    "        return ''.join([vocab.idx_to_token[i] for i in outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e37cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):  #@save\n",
    "    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        # Initialize the RNN layer \n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        return self.rnn(inputs, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a64cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(RNNLMScratch):  #@save\n",
    "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
    "    def init_params(self):\n",
    "        self.linear = nn.LazyLinear(self.vocab_size)\n",
    "\n",
    "    def output_layer(self, hiddens):\n",
    "        return self.linear(hiddens).swapaxes(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53165170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, criterion, optimizer, num_epochs=100, grad_clip=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on {device}\")\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, Y in data.get_dataloader(train=True):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[-1]), Y.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data.get_dataloader(train=True))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in data.get_dataloader(train=False):\n",
    "                X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "                val_outputs = model(X_val)\n",
    "                v_loss = criterion(val_outputs.reshape(-1, val_outputs.shape[-1]), Y_val.reshape(-1))\n",
    "                val_loss += v_loss.item()\n",
    "        avg_val_loss = val_loss / len(data.get_dataloader(train=False))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b30c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, root='../data'):\n",
    "        self.root = root\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                           shuffle=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff732286",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f7e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens  \n",
    "        self.sigma = sigma\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                        device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                        device=inputs.device)\n",
    "        else:\n",
    "            H, C = H_C\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
    "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
    "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
    "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                            torch.matmul(H, self.W_hc) + self.b_c)\n",
    "            C = F * C + I * C_tilde\n",
    "            H = O * torch.tanh(C)\n",
    "            outputs.append(H)\n",
    "        return outputs, (H, C)\n",
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e335395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 2.7175, Val Loss: 2.6605\n",
      "Epoch 20, Train Loss: 2.3602, Val Loss: 2.3193\n",
      "Epoch 30, Train Loss: 2.1932, Val Loss: 2.2437\n",
      "Epoch 40, Train Loss: 2.0721, Val Loss: 2.1293\n",
      "Epoch 50, Train Loss: 1.9519, Val Loss: 2.0443\n",
      "Epoch 60, Train Loss: 1.8565, Val Loss: 2.0358\n",
      "Epoch 70, Train Loss: 1.7631, Val Loss: 1.9534\n",
      "Epoch 80, Train Loss: 1.6781, Val Loss: 1.9551\n",
      "Epoch 90, Train Loss: 1.6153, Val Loss: 1.9478\n",
      "Epoch 100, Train Loss: 1.5642, Val Loss: 1.9615\n",
      "Epoch 110, Train Loss: 1.5301, Val Loss: 1.9727\n",
      "Epoch 120, Train Loss: 1.4865, Val Loss: 1.9878\n",
      "Epoch 130, Train Loss: 1.4570, Val Loss: 2.0108\n",
      "Epoch 140, Train Loss: 1.4241, Val Loss: 2.0013\n",
      "Epoch 150, Train Loss: 1.3861, Val Loss: 2.0358\n",
      "Epoch 160, Train Loss: 1.3667, Val Loss: 2.0282\n",
      "Epoch 170, Train Loss: 1.3336, Val Loss: 2.0574\n",
      "Epoch 180, Train Loss: 1.3324, Val Loss: 2.0956\n",
      "Epoch 190, Train Loss: 1.3139, Val Loss: 2.1019\n",
      "Epoch 200, Train Loss: 1.2999, Val Loss: 2.0886\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e6415d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has of the time travell'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661e30",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7637454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        return self.rnn(inputs, H_C)\n",
    "\n",
    "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d6c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 2.4301, Val Loss: 2.4004\n",
      "Epoch 20, Train Loss: 2.1455, Val Loss: 2.2037\n",
      "Epoch 30, Train Loss: 2.0205, Val Loss: 2.0649\n",
      "Epoch 40, Train Loss: 1.8780, Val Loss: 2.0414\n",
      "Epoch 50, Train Loss: 1.7798, Val Loss: 1.9965\n",
      "Epoch 60, Train Loss: 1.6860, Val Loss: 1.9929\n",
      "Epoch 70, Train Loss: 1.6362, Val Loss: 1.9879\n",
      "Epoch 80, Train Loss: 1.5686, Val Loss: 2.0155\n",
      "Epoch 90, Train Loss: 1.5354, Val Loss: 1.9819\n",
      "Epoch 100, Train Loss: 1.4912, Val Loss: 1.9927\n",
      "Epoch 110, Train Loss: 1.4480, Val Loss: 2.0496\n",
      "Epoch 120, Train Loss: 1.4335, Val Loss: 2.0229\n",
      "Epoch 130, Train Loss: 1.4191, Val Loss: 2.0452\n",
      "Epoch 140, Train Loss: 1.3762, Val Loss: 2.0287\n",
      "Epoch 150, Train Loss: 1.3575, Val Loss: 2.0636\n",
      "Epoch 160, Train Loss: 1.3426, Val Loss: 2.0870\n",
      "Epoch 170, Train Loss: 1.3227, Val Loss: 2.0914\n",
      "Epoch 180, Train Loss: 1.3166, Val Loss: 2.1021\n",
      "Epoch 190, Train Loss: 1.2948, Val Loss: 2.1037\n",
      "Epoch 200, Train Loss: 1.2720, Val Loss: 2.1632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'it has youns and mension a'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "model = RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)\n",
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394bef5",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81feb5",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daeb4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        if H is None:\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens), device=inputs.device)\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +\n",
    "                              torch.matmul(H, self.W_hz) + self.b_z)\n",
    "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                              torch.matmul(H, self.W_hr) + self.b_r)\n",
    "            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                                 torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "            H = Z * H + (1 - Z) * H_tilde\n",
    "            outputs.append(H)\n",
    "        outputs = torch.stack(outputs)  # Stack list into tensor\n",
    "        return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5fdc98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 2.2851, Val Loss: 2.2562\n",
      "Epoch 20, Train Loss: 2.0607, Val Loss: 2.1652\n",
      "Epoch 30, Train Loss: 1.9018, Val Loss: 2.0258\n",
      "Epoch 40, Train Loss: 1.7926, Val Loss: 1.9965\n",
      "Epoch 50, Train Loss: 1.6673, Val Loss: 1.9585\n",
      "Epoch 60, Train Loss: 1.5773, Val Loss: 1.9623\n",
      "Epoch 70, Train Loss: 1.5252, Val Loss: 1.9864\n",
      "Epoch 80, Train Loss: 1.4635, Val Loss: 2.0156\n",
      "Epoch 90, Train Loss: 1.4318, Val Loss: 2.0521\n",
      "Epoch 100, Train Loss: 1.4042, Val Loss: 2.0542\n",
      "Epoch 110, Train Loss: 1.3697, Val Loss: 2.0964\n",
      "Epoch 120, Train Loss: 1.3511, Val Loss: 2.0894\n",
      "Epoch 130, Train Loss: 1.3249, Val Loss: 2.1302\n",
      "Epoch 140, Train Loss: 1.2952, Val Loss: 2.1508\n",
      "Epoch 150, Train Loss: 1.2992, Val Loss: 2.1484\n",
      "Epoch 160, Train Loss: 1.3023, Val Loss: 2.1509\n",
      "Epoch 170, Train Loss: 1.2698, Val Loss: 2.1830\n",
      "Epoch 180, Train Loss: 1.2506, Val Loss: 2.1686\n",
      "Epoch 190, Train Loss: 1.2599, Val Loss: 2.2371\n",
      "Epoch 200, Train Loss: 1.2600, Val Loss: 2.2635\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gruscratch = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(gruscratch, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820896c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has thing to the time t'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687053c5",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a7dd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c3da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 2.2407, Val Loss: 2.2230\n",
      "Epoch 20, Train Loss: 1.9903, Val Loss: 2.0666\n",
      "Epoch 30, Train Loss: 1.8522, Val Loss: 2.0428\n",
      "Epoch 40, Train Loss: 1.7271, Val Loss: 1.9647\n",
      "Epoch 50, Train Loss: 1.6330, Val Loss: 1.9423\n",
      "Epoch 60, Train Loss: 1.5688, Val Loss: 1.9671\n",
      "Epoch 70, Train Loss: 1.4939, Val Loss: 1.9882\n",
      "Epoch 80, Train Loss: 1.4751, Val Loss: 2.0005\n",
      "Epoch 90, Train Loss: 1.4237, Val Loss: 2.0276\n",
      "Epoch 100, Train Loss: 1.4097, Val Loss: 2.0526\n",
      "Epoch 110, Train Loss: 1.3964, Val Loss: 2.0987\n",
      "Epoch 120, Train Loss: 1.3713, Val Loss: 2.0640\n",
      "Epoch 130, Train Loss: 1.3459, Val Loss: 2.0612\n",
      "Epoch 140, Train Loss: 1.3410, Val Loss: 2.0900\n",
      "Epoch 150, Train Loss: 1.3395, Val Loss: 2.1356\n",
      "Epoch 160, Train Loss: 1.3183, Val Loss: 2.1181\n",
      "Epoch 170, Train Loss: 1.2905, Val Loss: 2.0967\n",
      "Epoch 180, Train Loss: 1.2750, Val Loss: 2.1060\n",
      "Epoch 190, Train Loss: 1.2938, Val Loss: 2.1364\n",
      "Epoch 200, Train Loss: 1.2850, Val Loss: 2.1280\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90ade89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has the time traveller '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62242edc",
   "metadata": {},
   "source": [
    "# Deep Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c94a80",
   "metadata": {},
   "source": [
    "# Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160d442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNNScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.sigma = sigma\n",
    "        self.rnns = nn.Sequential(*[RNNScratch(\n",
    "            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)\n",
    "                                    for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        outputs = inputs\n",
    "        if Hs is None: Hs = [None] * self.num_layers\n",
    "        for i in range(self.num_layers):\n",
    "            outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n",
    "            outputs = torch.stack(outputs, 0)\n",
    "        return outputs, Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f1dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 2.8423, Val Loss: 2.8214\n",
      "Epoch 20, Train Loss: 2.8368, Val Loss: 2.8212\n",
      "Epoch 30, Train Loss: 2.5868, Val Loss: 2.5858\n",
      "Epoch 40, Train Loss: 2.2705, Val Loss: 2.2876\n",
      "Epoch 50, Train Loss: 2.1398, Val Loss: 2.1948\n",
      "Epoch 60, Train Loss: 1.9977, Val Loss: 2.0891\n",
      "Epoch 70, Train Loss: 1.9052, Val Loss: 2.0595\n",
      "Epoch 80, Train Loss: 1.8268, Val Loss: 2.0986\n",
      "Epoch 90, Train Loss: 1.7891, Val Loss: 2.0449\n",
      "Epoch 100, Train Loss: 1.6818, Val Loss: 2.0425\n",
      "Epoch 110, Train Loss: 1.6570, Val Loss: 2.0561\n",
      "Epoch 120, Train Loss: 1.5920, Val Loss: 2.0450\n",
      "Epoch 130, Train Loss: 1.6088, Val Loss: 2.0517\n",
      "Epoch 140, Train Loss: 1.5335, Val Loss: 2.0535\n",
      "Epoch 150, Train Loss: 1.5011, Val Loss: 2.0876\n",
      "Epoch 160, Train Loss: 1.4789, Val Loss: 2.0725\n",
      "Epoch 170, Train Loss: 1.4981, Val Loss: 2.1358\n",
      "Epoch 180, Train Loss: 1.4629, Val Loss: 2.1198\n",
      "Epoch 190, Train Loss: 1.4526, Val Loss: 2.1216\n",
      "Epoch 200, Train Loss: 1.4735, Val Loss: 2.0824\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n",
    "                              num_hiddens=32, num_layers=2)\n",
    "model = RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ad7e27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has of the proved the p'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a4aa2",
   "metadata": {},
   "source": [
    "# Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "442eba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\n",
    "                          dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aabdd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 2.6735, Val Loss: 2.5305\n",
      "Epoch 20, Train Loss: 2.2897, Val Loss: 2.2963\n",
      "Epoch 30, Train Loss: 2.1237, Val Loss: 2.1621\n",
      "Epoch 40, Train Loss: 1.9957, Val Loss: 2.1063\n",
      "Epoch 50, Train Loss: 1.8789, Val Loss: 2.0036\n",
      "Epoch 60, Train Loss: 1.7641, Val Loss: 1.9516\n",
      "Epoch 70, Train Loss: 1.6722, Val Loss: 1.9261\n",
      "Epoch 80, Train Loss: 1.5723, Val Loss: 1.9102\n",
      "Epoch 90, Train Loss: 1.4981, Val Loss: 1.9337\n",
      "Epoch 100, Train Loss: 1.4297, Val Loss: 1.9403\n",
      "Epoch 110, Train Loss: 1.3680, Val Loss: 1.9632\n",
      "Epoch 120, Train Loss: 1.3282, Val Loss: 2.0108\n",
      "Epoch 130, Train Loss: 1.2738, Val Loss: 2.0772\n",
      "Epoch 140, Train Loss: 1.2383, Val Loss: 2.0972\n",
      "Epoch 150, Train Loss: 1.1791, Val Loss: 2.1531\n",
      "Epoch 160, Train Loss: 1.1384, Val Loss: 2.1647\n",
      "Epoch 170, Train Loss: 1.1293, Val Loss: 2.2357\n",
      "Epoch 180, Train Loss: 1.0942, Val Loss: 2.2575\n",
      "Epoch 190, Train Loss: 1.0686, Val Loss: 2.3129\n",
      "Epoch 200, Train Loss: 1.0260, Val Loss: 2.3763\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\n",
    "model = RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a73e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has or miner the lince '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be82c0c",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e8b96",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c371f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "        self.f_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled\n",
    "    \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "        f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "        b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "        outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
    "            f_outputs, reversed(b_outputs))]\n",
    "        return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97943000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 0.9252, Val Loss: 0.7820\n",
      "Epoch 20, Train Loss: 0.1961, Val Loss: 0.1883\n",
      "Epoch 30, Train Loss: 0.1108, Val Loss: 0.1075\n",
      "Epoch 40, Train Loss: 0.0921, Val Loss: 0.0906\n",
      "Epoch 50, Train Loss: 0.0846, Val Loss: 0.0842\n",
      "Epoch 60, Train Loss: 0.0805, Val Loss: 0.0810\n",
      "Epoch 70, Train Loss: 0.0780, Val Loss: 0.0789\n",
      "Epoch 80, Train Loss: 0.0762, Val Loss: 0.0775\n",
      "Epoch 90, Train Loss: 0.0748, Val Loss: 0.0765\n",
      "Epoch 100, Train Loss: 0.0739, Val Loss: 0.0756\n",
      "Epoch 110, Train Loss: 0.0731, Val Loss: 0.0749\n",
      "Epoch 120, Train Loss: 0.0724, Val Loss: 0.0743\n",
      "Epoch 130, Train Loss: 0.0718, Val Loss: 0.0738\n",
      "Epoch 140, Train Loss: 0.0713, Val Loss: 0.0734\n",
      "Epoch 150, Train Loss: 0.0709, Val Loss: 0.0730\n",
      "Epoch 160, Train Loss: 0.0705, Val Loss: 0.0726\n",
      "Epoch 170, Train Loss: 0.0702, Val Loss: 0.0723\n",
      "Epoch 180, Train Loss: 0.0699, Val Loss: 0.0721\n",
      "Epoch 190, Train Loss: 0.0696, Val Loss: 0.0718\n",
      "Epoch 200, Train Loss: 0.0693, Val Loss: 0.0715\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "birnn = BiRNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(birnn, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1677ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it hasasasasasasasasasasas'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f611a1",
   "metadata": {},
   "source": [
    "## Consise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59cc0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__(num_inputs=num_inputs, num_hiddens=num_hiddens)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = 0.01\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled\n",
    "\n",
    "    def forward(self, inputs, Hs=None):\n",
    "        return self.rnn(inputs, Hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f69cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 10, Train Loss: 0.9109, Val Loss: 0.7642\n",
      "Epoch 20, Train Loss: 0.1980, Val Loss: 0.1908\n",
      "Epoch 30, Train Loss: 0.1170, Val Loss: 0.1165\n",
      "Epoch 40, Train Loss: 0.0963, Val Loss: 0.0972\n",
      "Epoch 50, Train Loss: 0.0874, Val Loss: 0.0889\n",
      "Epoch 60, Train Loss: 0.0824, Val Loss: 0.0844\n",
      "Epoch 70, Train Loss: 0.0792, Val Loss: 0.0815\n",
      "Epoch 80, Train Loss: 0.0771, Val Loss: 0.0796\n",
      "Epoch 90, Train Loss: 0.0756, Val Loss: 0.0781\n",
      "Epoch 100, Train Loss: 0.0744, Val Loss: 0.0770\n",
      "Epoch 110, Train Loss: 0.0735, Val Loss: 0.0761\n",
      "Epoch 120, Train Loss: 0.0727, Val Loss: 0.0754\n",
      "Epoch 130, Train Loss: 0.0721, Val Loss: 0.0748\n",
      "Epoch 140, Train Loss: 0.0715, Val Loss: 0.0743\n",
      "Epoch 150, Train Loss: 0.0710, Val Loss: 0.0738\n",
      "Epoch 160, Train Loss: 0.0706, Val Loss: 0.0734\n",
      "Epoch 170, Train Loss: 0.0703, Val Loss: 0.0731\n",
      "Epoch 180, Train Loss: 0.0699, Val Loss: 0.0728\n",
      "Epoch 190, Train Loss: 0.0696, Val Loss: 0.0725\n",
      "Epoch 200, Train Loss: 0.0694, Val Loss: 0.0722\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "BiGRU = BiGRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(BiGRU, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5de64cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it hasasasasasasasasasasas'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evice = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f32d46",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2293529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "reduce_sum = lambda x, axis: x.sum(axis=axis, keepdim=True)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "\n",
    "class MTFraEng(DataModule):\n",
    "    \"\"\"The English-French dataset.\n",
    "\n",
    "    Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "    def _download(self):\n",
    "        with open('fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "               for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "\n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            if max_examples and i > max_examples: break\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                # Skip empty tokens\n",
    "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "        return src, tgt\n",
    "\n",
    "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "        super(MTFraEng, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "            self._download())\n",
    "\n",
    "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            pad_or_trim = lambda seq, t: (\n",
    "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "            if is_tgt:\n",
    "                sentences = [['<bos>'] + s for s in sentences]\n",
    "            if vocab is None:\n",
    "                vocab = Vocab(sentences, min_freq=2)\n",
    "            array = torch.tensor([vocab[s] for s in sentences])\n",
    "            valid_len = reduce_sum(\n",
    "                astype(array != vocab['<pad>'], torch.int32), 1)\n",
    "            return array, vocab, valid_len\n",
    "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                                  self.num_train + self.num_val)\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "                src_vocab, tgt_vocab)\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
    "        idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "    def build(self, src_sentences, tgt_sentences):\n",
    "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
    "        raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "            src_sentences, tgt_sentences)])\n",
    "        arrays, _, _ = self._build_arrays(\n",
    "            raw_text, self.src_vocab, self.tgt_vocab)\n",
    "        return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "092de7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Translation\n",
    "\n",
    "#Reduce sum\n",
    "reduce_sum = lambda x, axis: x.sum(axis=axis, keepdim=True)\n",
    "#astype\n",
    "astype = lambda x, y: x.type(y)\n",
    "\n",
    "class MTFraEng(DataModule):\n",
    "    def _download(self):\n",
    "        with open('fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "               for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "\n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            if max_examples and i > max_examples: break\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                # Skip empty tokens\n",
    "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "        return src, tgt\n",
    "\n",
    "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "        super(MTFraEng, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "            self._download())\n",
    "\n",
    "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            pad_or_trim = lambda seq, t: (\n",
    "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "            if is_tgt:\n",
    "                sentences = [['<bos>'] + s for s in sentences]\n",
    "            if vocab is None:\n",
    "                vocab = Vocab(sentences, min_freq=2)\n",
    "            array = torch.tensor([vocab[s] for s in sentences])\n",
    "            # Fix: Ensure array is always 2D\n",
    "            if array.dim() == 1:\n",
    "                array = array.unsqueeze(0)  # Add batch dimension\n",
    "            valid_len = reduce_sum(\n",
    "                astype(array != vocab['<pad>'], torch.int32), 1)\n",
    "            return array, vocab, valid_len\n",
    "        \n",
    "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                                self.num_train + self.num_val)\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "        \n",
    "        # Fix: Ensure both arrays are 2D before slicing\n",
    "        if tgt_array.dim() == 1:\n",
    "            tgt_array = tgt_array.unsqueeze(0)\n",
    "        if src_array.dim() == 1:\n",
    "            src_array = src_array.unsqueeze(0)\n",
    "        \n",
    "        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "                src_vocab, tgt_vocab)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "    def build(self, src_sentences, tgt_sentences):\n",
    "        raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "            src_sentences, tgt_sentences)])\n",
    "        arrays, _, _ = self._build_arrays(\n",
    "            raw_text, self.src_vocab, self.tgt_vocab)\n",
    "        return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27636066",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MTFraEng(batch_size=3, num_steps=9, num_train=100, num_val=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8b8dc",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5e0a4",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c250ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  \n",
    "    \"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65274dbe",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "557a1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module): \n",
    "    \"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243233a2",
   "metadata": {},
   "source": [
    "## Putting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24aba344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"The base class for the encoder--decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9f7f1",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence Learning For Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be1ade",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a9a463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq2seq(module): \n",
    "    \"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "         nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "\n",
    "class Seq2SeqEncoder(Encoder): \n",
    "    \"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(X.t().type(torch.int64))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "baa48171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected shape: (9, 4, 16)\n",
      "Actual shape: torch.Size([9, 4, 16])\n",
      "Shape match: True\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 9\n",
    "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "X = torch.zeros((batch_size, num_steps))\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "print(f\"Expected shape: ({num_steps}, {batch_size}, {num_hiddens})\")\n",
    "print(f\"Actual shape: {enc_outputs.shape}\")\n",
    "print(f\"Shape match: {enc_outputs.shape == (num_steps, batch_size, num_hiddens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f8665",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96deaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size+num_hiddens, num_hiddens,\n",
    "                           num_layers, dropout=dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(X.t().type(torch.int32))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = self.dense(outputs).swapaxes(0, 1)\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f4e8730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected shape: (4, 9, 10)\n",
      "Actual shape: torch.Size([4, 9, 10])\n",
      "Shape match: True\n",
      "Expected shape: (2, 4, 16)\n",
      "Actual shape: torch.Size([2, 4, 16])\n",
      "Shape match: True\n"
     ]
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "state = decoder.init_state(encoder(X))\n",
    "dec_outputs, state = decoder(X, state)\n",
    "print(f\"Expected shape: ({batch_size}, {num_steps}, {vocab_size})\")\n",
    "print(f\"Actual shape: {dec_outputs.shape}\")\n",
    "print(f\"Shape match: {dec_outputs.shape == (batch_size, num_steps, vocab_size)}\")\n",
    "print(f\"Expected shape: ({num_layers}, {batch_size}, {num_hiddens})\")\n",
    "print(f\"Actual shape: {state[1].shape}\")\n",
    "print(f\"Shape match: {state[1].shape == (num_layers, batch_size, num_hiddens)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1468050",
   "metadata": {},
   "source": [
    "## Encoder-Decoder for Sequence-to-Sequence Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "738452ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(EncoderDecoder): \n",
    "    \"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
    "        super().__init__(encoder, decoder)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_pad = tgt_pad\n",
    "        self.lr = lr\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer is used here\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # Calculate cross-entropy loss directly\n",
    "        l = nn.CrossEntropyLoss(reduction='none')(Y_hat.reshape(-1, Y_hat.shape[-1]), Y.reshape(-1))\n",
    "        mask = (Y.reshape(-1) != self.tgt_pad).type(torch.float32)\n",
    "        return (l * mask).sum() / mask.sum()\n",
    "\n",
    "    def predict_step(self, batch, device, num_steps,\n",
    "                 save_attention_weights=False):\n",
    "        batch = [a.to(device) for a in batch]\n",
    "        src, tgt, src_valid_len, _ = batch\n",
    "        enc_all_outputs = self.encoder(src, src_valid_len)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
    "        outputs, attention_weights = [tgt[:, (0)].unsqueeze(1), ], []\n",
    "        for _ in range(num_steps):\n",
    "            Y, dec_state = self.decoder(outputs[-1], dec_state)\n",
    "            outputs.append(Y.argmax(2))\n",
    "            # Save attention weights (to be covered later)\n",
    "            if save_attention_weights:\n",
    "                attention_weights.append(self.decoder.attention_weights)\n",
    "        return torch.cat(outputs[1:], 1), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76538953",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MTFraEng(batch_size=32, num_steps=9, num_train=100, num_val=20)\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
    "                lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0a387f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Training samples: 0\n",
      "Validation samples: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     51\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 53\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_arrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     56\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Custom training loop for sequence-to-sequence model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on {device}\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# Get the data arrays directly\n",
    "train_arrays = data.arrays[True]\n",
    "val_arrays = data.arrays[False]\n",
    "\n",
    "# Fix: Unpack if nested\n",
    "if len(train_arrays) == 1 and isinstance(train_arrays[0], tuple):\n",
    "    train_arrays = train_arrays[0]\n",
    "    val_arrays = val_arrays[0]\n",
    "\n",
    "print(f\"Training samples: {len(train_arrays[0])}\")\n",
    "print(f\"Validation samples: {len(val_arrays[0])}\")\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process training data in batches\n",
    "    for i in range(0, len(train_arrays[0]), data.batch_size):\n",
    "        # Get batch\n",
    "        batch_src = train_arrays[0][i:i+data.batch_size]\n",
    "        batch_tgt = train_arrays[1][i:i+data.batch_size]\n",
    "        batch_src_valid_len = train_arrays[2][i:i+data.batch_size]\n",
    "        batch_tgt_shifted = train_arrays[3][i:i+data.batch_size]\n",
    "        \n",
    "        # Move to device\n",
    "        src = batch_src.to(device)\n",
    "        tgt = batch_tgt.to(device)\n",
    "        src_valid_len = batch_src_valid_len.to(device)\n",
    "        tgt_shifted = batch_tgt_shifted.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(src, tgt)\n",
    "        \n",
    "        # Calculate loss manually\n",
    "        l = criterion(outputs.reshape(-1, outputs.shape[-1]), tgt_shifted.reshape(-1))\n",
    "        mask = (tgt_shifted.reshape(-1) != model.tgt_pad).type(torch.float32)\n",
    "        loss = (l * mask).sum() / mask.sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (len(train_arrays[0]) // data.batch_size)\n",
    "\n",
    "    # Validation\n",
    "    avg_val_loss = 0.0\n",
    "    if len(val_arrays[0]) > 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(val_arrays[0]), data.batch_size):\n",
    "                # Get validation batch\n",
    "                batch_src_val = val_arrays[0][i:i+data.batch_size]\n",
    "                batch_tgt_val = val_arrays[1][i:i+data.batch_size]\n",
    "                batch_src_valid_len_val = val_arrays[2][i:i+data.batch_size]\n",
    "                batch_tgt_shifted_val = val_arrays[3][i:i+data.batch_size]\n",
    "                \n",
    "                # Move to device\n",
    "                src_val = batch_src_val.to(device)\n",
    "                tgt_val = batch_tgt_val.to(device)\n",
    "                src_valid_len_val = batch_src_valid_len_val.to(device)\n",
    "                tgt_shifted_val = batch_tgt_shifted_val.to(device)\n",
    "                \n",
    "                val_outputs = model(src_val, tgt_val)\n",
    "                \n",
    "                # Calculate validation loss manually\n",
    "                v_l = criterion(val_outputs.reshape(-1, val_outputs.shape[-1]), tgt_shifted_val.reshape(-1))\n",
    "                v_mask = (tgt_shifted_val.reshape(-1) != model.tgt_pad).type(torch.float32)\n",
    "                v_loss = (v_l * v_mask).sum() / v_mask.sum()\n",
    "                \n",
    "                val_loss += v_loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / (len(val_arrays[0]) // data.batch_size)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf79766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"Compute the BLEU.\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, min(k, len_pred) + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33fcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'], bleu,0.000\n",
      "i lost . => ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'], bleu,0.000\n",
      "he's calm . => ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'], bleu,0.000\n",
      "i'm home . => ['<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'], bleu,0.000\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', 'i lost .', 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "preds, _ = model.predict_step(\n",
    "    data.build(engs, fras), 'cuda', data.num_steps)\n",
    "for en, fr, p in zip(engs, fras, preds):\n",
    "    translation = []\n",
    "    for token in data.tgt_vocab.to_tokens(p):\n",
    "        if token == '<eos>':\n",
    "            break\n",
    "        translation.append(token)\n",
    "    print(f'{en} => {translation}, bleu,'\n",
    "          f'{bleu(\" \".join(translation), fr, k=2):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
