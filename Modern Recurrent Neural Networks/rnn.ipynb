{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ecb66b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025e9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e6fae",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d43cac",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae04e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "text = \"\"\n",
    "full_text = \"\"\n",
    "with open('The Time Machine - Sample.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "    full_text = preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94d4f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "    \n",
    "def tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "def build(raw_text, vocab=None):\n",
    "    tokens = tokenize(preprocess(raw_text))\n",
    "    if vocab is None: vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for token in tokens]\n",
    "    return corpus, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c77930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageData(nn.Module):\n",
    "    def _download(self):\n",
    "        fname = \"The Time Machine - Sample.txt\"\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        return list(text)\n",
    "\n",
    "    def build(self, raw_text, vocab=None):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        tokens = self._tokenize(self._preprocess(raw_text))\n",
    "        if vocab is None: vocab = Vocab(tokens)\n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "        \"\"\"Defined in :numref:`sec_language-model`\"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        corpus, self.vocab = self.build(self._download())\n",
    "        array = torch.tensor([corpus[i:i+num_steps+1]\n",
    "                            for i in range(len(corpus)-num_steps)])\n",
    "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"Defined in :numref:`subsec_partitioning-seqs`\"\"\"\n",
    "        idx = slice(0, self.num_train) if train else slice(\n",
    "            self.num_train, self.num_train + self.num_val)\n",
    "        return self.get_tensorloader([self.X, self.Y], train, idx)\n",
    "    \n",
    "    def get_tensorloader(self, tensors, train, idx_slice):\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            tensors[0][idx_slice], tensors[1][idx_slice]\n",
    "        )\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=train\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61fe75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(nn.Module):\n",
    "    \"\"\"The RNN model implemented from scratch.\n",
    "\n",
    "    Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "        self.W_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.W_hh = nn.Parameter(\n",
    "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        if state is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                              device=inputs.device)\n",
    "        else:\n",
    "            state, = state\n",
    "        outputs = []\n",
    "        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
    "            state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                             torch.matmul(state, self.W_hh) + self.b_h)\n",
    "            outputs.append(state)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72caae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLMScratch(nn.Module): \n",
    "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lr = lr\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=False)\n",
    "    \n",
    "    def one_hot(self, X):\n",
    "        # Output shape: (num_steps, batch_size, vocab_size)\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
    "        return torch.stack(outputs, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clip_gradients(self, grad_clip_val, model):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "        if norm > grad_clip_val:\n",
    "            for param in params:\n",
    "                param.grad[:] *= grad_clip_val / norm\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        embs = self.one_hot(X)\n",
    "        rnn_outputs, _ = self.rnn(embs, state)\n",
    "        return self.output_layer(rnn_outputs)\n",
    "\n",
    "    def predict(self, prefix, num_preds, vocab, device=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        state, outputs = None, [vocab[prefix[0]]]\n",
    "        for i in range(len(prefix) + num_preds - 1):\n",
    "            X = torch.tensor([[outputs[-1]]], device=device)\n",
    "            embs = self.one_hot(X)\n",
    "            rnn_outputs, state = self.rnn(embs, state)\n",
    "            if i < len(prefix) - 1:  # Warm-up period\n",
    "                outputs.append(vocab[prefix[i + 1]])\n",
    "            else:  # Predict num_preds steps\n",
    "                Y = self.output_layer(rnn_outputs)\n",
    "                outputs.append(int(torch.reshape(torch.argmax(Y, axis=2), (1,))))\n",
    "        return ''.join([vocab.idx_to_token[i] for i in outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e37cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):  #@save\n",
    "    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        # Initialize the RNN layer \n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        return self.rnn(inputs, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a64cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(RNNLMScratch):  #@save\n",
    "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
    "    def init_params(self):\n",
    "        self.linear = nn.LazyLinear(self.vocab_size)\n",
    "\n",
    "    def output_layer(self, hiddens):\n",
    "        return self.linear(hiddens).swapaxes(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53165170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, criterion, optimizer, num_epochs=100, grad_clip=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, Y in data.get_dataloader(train=True):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[-1]), Y.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data.get_dataloader(train=True))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in data.get_dataloader(train=False):\n",
    "                X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "                val_outputs = model(X_val)\n",
    "                v_loss = criterion(val_outputs.reshape(-1, val_outputs.shape[-1]), Y_val.reshape(-1))\n",
    "                val_loss += v_loss.item()\n",
    "        avg_val_loss = val_loss / len(data.get_dataloader(train=False))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b30c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(self, root='../data'):\n",
    "        self.root = root\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size,\n",
    "                                           shuffle=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff732286",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f7e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens  \n",
    "        self.sigma = sigma\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                        device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                        device=inputs.device)\n",
    "        else:\n",
    "            H, C = H_C\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
    "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
    "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
    "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                            torch.matmul(H, self.W_hc) + self.b_c)\n",
    "            C = F * C + I * C_tilde\n",
    "            H = O * torch.tanh(C)\n",
    "            outputs.append(H)\n",
    "        return outputs, (H, C)\n",
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e335395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.7088, Val Loss: 2.6571\n",
      "Epoch 20, Train Loss: 2.3641, Val Loss: 2.3168\n",
      "Epoch 30, Train Loss: 2.1869, Val Loss: 2.2027\n",
      "Epoch 40, Train Loss: 2.0476, Val Loss: 2.1019\n",
      "Epoch 50, Train Loss: 1.9135, Val Loss: 2.0197\n",
      "Epoch 60, Train Loss: 1.8205, Val Loss: 1.9956\n",
      "Epoch 70, Train Loss: 1.7222, Val Loss: 1.9564\n",
      "Epoch 80, Train Loss: 1.6446, Val Loss: 1.9523\n",
      "Epoch 90, Train Loss: 1.5915, Val Loss: 1.9457\n",
      "Epoch 100, Train Loss: 1.5285, Val Loss: 1.9630\n",
      "Epoch 110, Train Loss: 1.4944, Val Loss: 1.9349\n",
      "Epoch 120, Train Loss: 1.4629, Val Loss: 1.9636\n",
      "Epoch 130, Train Loss: 1.4284, Val Loss: 1.9909\n",
      "Epoch 140, Train Loss: 1.3971, Val Loss: 2.0054\n",
      "Epoch 150, Train Loss: 1.3635, Val Loss: 2.0093\n",
      "Epoch 160, Train Loss: 1.3501, Val Loss: 2.0311\n",
      "Epoch 170, Train Loss: 1.3215, Val Loss: 2.0382\n",
      "Epoch 180, Train Loss: 1.3067, Val Loss: 2.0505\n",
      "Epoch 190, Train Loss: 1.2822, Val Loss: 2.0683\n",
      "Epoch 200, Train Loss: 1.2731, Val Loss: 2.1052\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e6415d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has a so in the german '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661e30",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7637454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        return self.rnn(inputs, H_C)\n",
    "\n",
    "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d6c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.4146, Val Loss: 2.3773\n",
      "Epoch 20, Train Loss: 2.1465, Val Loss: 2.1725\n",
      "Epoch 30, Train Loss: 1.9817, Val Loss: 2.0954\n",
      "Epoch 40, Train Loss: 1.8703, Val Loss: 2.0158\n",
      "Epoch 50, Train Loss: 1.7800, Val Loss: 1.9962\n",
      "Epoch 60, Train Loss: 1.7051, Val Loss: 1.9677\n",
      "Epoch 70, Train Loss: 1.6464, Val Loss: 1.9678\n",
      "Epoch 80, Train Loss: 1.5794, Val Loss: 1.9685\n",
      "Epoch 90, Train Loss: 1.5441, Val Loss: 1.9715\n",
      "Epoch 100, Train Loss: 1.5053, Val Loss: 1.9587\n",
      "Epoch 110, Train Loss: 1.4541, Val Loss: 1.9722\n",
      "Epoch 120, Train Loss: 1.4376, Val Loss: 1.9987\n",
      "Epoch 130, Train Loss: 1.4158, Val Loss: 2.0073\n",
      "Epoch 140, Train Loss: 1.4006, Val Loss: 2.0105\n",
      "Epoch 150, Train Loss: 1.3763, Val Loss: 2.0418\n",
      "Epoch 160, Train Loss: 1.3647, Val Loss: 2.0605\n",
      "Epoch 170, Train Loss: 1.3282, Val Loss: 2.0384\n",
      "Epoch 180, Train Loss: 1.3246, Val Loss: 2.0884\n",
      "Epoch 190, Train Loss: 1.3007, Val Loss: 2.0760\n",
      "Epoch 200, Train Loss: 1.2944, Val Loss: 2.0749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'it has of a mather an and '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "model = RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)\n",
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394bef5",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81feb5",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daeb4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        if H is None:\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens), device=inputs.device)\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +\n",
    "                              torch.matmul(H, self.W_hz) + self.b_z)\n",
    "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                              torch.matmul(H, self.W_hr) + self.b_r)\n",
    "            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                                 torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "            H = Z * H + (1 - Z) * H_tilde\n",
    "            outputs.append(H)\n",
    "        outputs = torch.stack(outputs)  # Stack list into tensor\n",
    "        return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5fdc98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.2841, Val Loss: 2.2779\n",
      "Epoch 20, Train Loss: 2.0408, Val Loss: 2.1516\n",
      "Epoch 30, Train Loss: 1.9200, Val Loss: 2.0534\n",
      "Epoch 40, Train Loss: 1.7732, Val Loss: 1.9736\n",
      "Epoch 50, Train Loss: 1.6670, Val Loss: 1.9513\n",
      "Epoch 60, Train Loss: 1.5774, Val Loss: 1.9577\n",
      "Epoch 70, Train Loss: 1.5210, Val Loss: 2.0112\n",
      "Epoch 80, Train Loss: 1.4570, Val Loss: 2.0196\n",
      "Epoch 90, Train Loss: 1.4177, Val Loss: 2.0410\n",
      "Epoch 100, Train Loss: 1.4048, Val Loss: 2.1077\n",
      "Epoch 110, Train Loss: 1.3668, Val Loss: 2.0886\n",
      "Epoch 120, Train Loss: 1.3337, Val Loss: 2.1117\n",
      "Epoch 130, Train Loss: 1.3406, Val Loss: 2.1284\n",
      "Epoch 140, Train Loss: 1.3075, Val Loss: 2.1511\n",
      "Epoch 150, Train Loss: 1.2880, Val Loss: 2.1690\n",
      "Epoch 160, Train Loss: 1.3116, Val Loss: 2.1951\n",
      "Epoch 170, Train Loss: 1.2695, Val Loss: 2.1903\n",
      "Epoch 180, Train Loss: 1.2634, Val Loss: 2.1893\n",
      "Epoch 190, Train Loss: 1.2309, Val Loss: 2.2347\n",
      "Epoch 200, Train Loss: 1.2331, Val Loss: 2.2156\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gruscratch = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(gruscratch, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "820896c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has a mite at sochight '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687053c5",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a7dd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c3da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.2376, Val Loss: 2.2204\n",
      "Epoch 20, Train Loss: 2.0194, Val Loss: 2.1080\n",
      "Epoch 30, Train Loss: 1.8511, Val Loss: 2.0052\n",
      "Epoch 40, Train Loss: 1.7170, Val Loss: 1.9654\n",
      "Epoch 50, Train Loss: 1.6247, Val Loss: 1.9545\n",
      "Epoch 60, Train Loss: 1.5354, Val Loss: 1.9874\n",
      "Epoch 70, Train Loss: 1.4950, Val Loss: 1.9949\n",
      "Epoch 80, Train Loss: 1.4493, Val Loss: 2.0399\n",
      "Epoch 90, Train Loss: 1.4365, Val Loss: 2.0811\n",
      "Epoch 100, Train Loss: 1.3947, Val Loss: 2.0711\n",
      "Epoch 110, Train Loss: 1.3799, Val Loss: 2.1144\n",
      "Epoch 120, Train Loss: 1.3501, Val Loss: 2.0949\n",
      "Epoch 130, Train Loss: 1.3411, Val Loss: 2.1111\n",
      "Epoch 140, Train Loss: 1.3268, Val Loss: 2.1653\n",
      "Epoch 150, Train Loss: 1.3219, Val Loss: 2.1604\n",
      "Epoch 160, Train Loss: 1.3250, Val Loss: 2.1541\n",
      "Epoch 170, Train Loss: 1.3151, Val Loss: 2.1527\n",
      "Epoch 180, Train Loss: 1.2928, Val Loss: 2.1824\n",
      "Epoch 190, Train Loss: 1.2841, Val Loss: 2.1603\n",
      "Epoch 200, Train Loss: 1.2782, Val Loss: 2.2223\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90ade89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has of course the prove'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62242edc",
   "metadata": {},
   "source": [
    "# Deep Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c94a80",
   "metadata": {},
   "source": [
    "# Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160d442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNNScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.sigma = sigma\n",
    "        self.rnns = nn.Sequential(*[RNNScratch(\n",
    "            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)\n",
    "                                    for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        outputs = inputs\n",
    "        if Hs is None: Hs = [None] * self.num_layers\n",
    "        for i in range(self.num_layers):\n",
    "            outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n",
    "            outputs = torch.stack(outputs, 0)\n",
    "        return outputs, Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f1dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.8396, Val Loss: 2.8229\n",
      "Epoch 20, Train Loss: 2.8276, Val Loss: 2.8007\n",
      "Epoch 30, Train Loss: 2.5537, Val Loss: 2.4737\n",
      "Epoch 40, Train Loss: 2.2566, Val Loss: 2.2551\n",
      "Epoch 50, Train Loss: 2.0935, Val Loss: 2.1659\n",
      "Epoch 60, Train Loss: 1.9976, Val Loss: 2.1216\n",
      "Epoch 70, Train Loss: 1.8881, Val Loss: 2.0663\n",
      "Epoch 80, Train Loss: 1.8163, Val Loss: 2.0127\n",
      "Epoch 90, Train Loss: 1.7493, Val Loss: 2.0306\n",
      "Epoch 100, Train Loss: 1.7006, Val Loss: 2.0811\n",
      "Epoch 110, Train Loss: 1.6356, Val Loss: 2.0553\n",
      "Epoch 120, Train Loss: 1.6043, Val Loss: 2.0981\n",
      "Epoch 130, Train Loss: 1.5676, Val Loss: 2.0388\n",
      "Epoch 140, Train Loss: 1.5728, Val Loss: 2.1009\n",
      "Epoch 150, Train Loss: 1.5041, Val Loss: 2.0374\n",
      "Epoch 160, Train Loss: 1.4739, Val Loss: 2.0400\n",
      "Epoch 170, Train Loss: 1.4937, Val Loss: 2.1322\n",
      "Epoch 180, Train Loss: 1.4868, Val Loss: 2.0960\n",
      "Epoch 190, Train Loss: 1.4463, Val Loss: 2.0914\n",
      "Epoch 200, Train Loss: 1.4511, Val Loss: 2.0952\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n",
    "                              num_hiddens=32, num_layers=2)\n",
    "model = RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ad7e27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it hasted the time travell'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a4aa2",
   "metadata": {},
   "source": [
    "# Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "442eba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\n",
    "                          dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aabdd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.6674, Val Loss: 2.6077\n",
      "Epoch 20, Train Loss: 2.2953, Val Loss: 2.2730\n",
      "Epoch 30, Train Loss: 2.1201, Val Loss: 2.1842\n",
      "Epoch 40, Train Loss: 2.0122, Val Loss: 2.0806\n",
      "Epoch 50, Train Loss: 1.8669, Val Loss: 2.0612\n",
      "Epoch 60, Train Loss: 1.7480, Val Loss: 1.9482\n",
      "Epoch 70, Train Loss: 1.6693, Val Loss: 1.9288\n",
      "Epoch 80, Train Loss: 1.5894, Val Loss: 1.9295\n",
      "Epoch 90, Train Loss: 1.5115, Val Loss: 1.9230\n",
      "Epoch 100, Train Loss: 1.4389, Val Loss: 1.9346\n",
      "Epoch 110, Train Loss: 1.3887, Val Loss: 1.9505\n",
      "Epoch 120, Train Loss: 1.3410, Val Loss: 1.9960\n",
      "Epoch 130, Train Loss: 1.2757, Val Loss: 2.0441\n",
      "Epoch 140, Train Loss: 1.2314, Val Loss: 2.0689\n",
      "Epoch 150, Train Loss: 1.1773, Val Loss: 2.1012\n",
      "Epoch 160, Train Loss: 1.1385, Val Loss: 2.1560\n",
      "Epoch 170, Train Loss: 1.1079, Val Loss: 2.1809\n",
      "Epoch 180, Train Loss: 1.1089, Val Loss: 2.2326\n",
      "Epoch 190, Train Loss: 1.0608, Val Loss: 2.2697\n",
      "Epoch 200, Train Loss: 1.0547, Val Loss: 2.3215\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\n",
    "model = RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a73e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has introduction is fou'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be82c0c",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e8b96",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c371f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNNScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "        self.f_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled\n",
    "    \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "        f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "        b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "        outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
    "            f_outputs, reversed(b_outputs))]\n",
    "        return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97943000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 1.5660, Val Loss: 1.0378\n",
      "Epoch 20, Train Loss: 0.2039, Val Loss: 0.1926\n",
      "Epoch 30, Train Loss: 0.1128, Val Loss: 0.1089\n",
      "Epoch 40, Train Loss: 0.0932, Val Loss: 0.0914\n",
      "Epoch 50, Train Loss: 0.0851, Val Loss: 0.0845\n",
      "Epoch 60, Train Loss: 0.0806, Val Loss: 0.0809\n",
      "Epoch 70, Train Loss: 0.0780, Val Loss: 0.0787\n",
      "Epoch 80, Train Loss: 0.0762, Val Loss: 0.0772\n",
      "Epoch 90, Train Loss: 0.0749, Val Loss: 0.0761\n",
      "Epoch 100, Train Loss: 0.0739, Val Loss: 0.0753\n",
      "Epoch 110, Train Loss: 0.0730, Val Loss: 0.0746\n",
      "Epoch 120, Train Loss: 0.0725, Val Loss: 0.0741\n",
      "Epoch 130, Train Loss: 0.0719, Val Loss: 0.0736\n",
      "Epoch 140, Train Loss: 0.0715, Val Loss: 0.0732\n",
      "Epoch 150, Train Loss: 0.0710, Val Loss: 0.0729\n",
      "Epoch 160, Train Loss: 0.0707, Val Loss: 0.0726\n",
      "Epoch 170, Train Loss: 0.0704, Val Loss: 0.0723\n",
      "Epoch 180, Train Loss: 0.0700, Val Loss: 0.0721\n",
      "Epoch 190, Train Loss: 0.0697, Val Loss: 0.0718\n",
      "Epoch 200, Train Loss: 0.0695, Val Loss: 0.0717\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "birnn = BiRNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(birnn, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1677ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it hasasasasasasasasasasas'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f611a1",
   "metadata": {},
   "source": [
    "## Consise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59cc0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__(num_inputs=num_inputs, num_hiddens=num_hiddens)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = 0.01\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled\n",
    "\n",
    "    def forward(self, inputs, Hs=None):\n",
    "        return self.rnn(inputs, Hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f69cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.9438, Val Loss: 0.7833\n",
      "Epoch 20, Train Loss: 0.1979, Val Loss: 0.1891\n",
      "Epoch 30, Train Loss: 0.1134, Val Loss: 0.1121\n",
      "Epoch 40, Train Loss: 0.0934, Val Loss: 0.0937\n",
      "Epoch 50, Train Loss: 0.0855, Val Loss: 0.0863\n",
      "Epoch 60, Train Loss: 0.0811, Val Loss: 0.0823\n",
      "Epoch 70, Train Loss: 0.0785, Val Loss: 0.0799\n",
      "Epoch 80, Train Loss: 0.0766, Val Loss: 0.0782\n",
      "Epoch 90, Train Loss: 0.0754, Val Loss: 0.0770\n",
      "Epoch 100, Train Loss: 0.0744, Val Loss: 0.0761\n",
      "Epoch 110, Train Loss: 0.0736, Val Loss: 0.0754\n",
      "Epoch 120, Train Loss: 0.0728, Val Loss: 0.0748\n",
      "Epoch 130, Train Loss: 0.0723, Val Loss: 0.0742\n",
      "Epoch 140, Train Loss: 0.0718, Val Loss: 0.0738\n",
      "Epoch 150, Train Loss: 0.0713, Val Loss: 0.0734\n",
      "Epoch 160, Train Loss: 0.0709, Val Loss: 0.0731\n",
      "Epoch 170, Train Loss: 0.0706, Val Loss: 0.0728\n",
      "Epoch 180, Train Loss: 0.0703, Val Loss: 0.0725\n",
      "Epoch 190, Train Loss: 0.0700, Val Loss: 0.0722\n",
      "Epoch 200, Train Loss: 0.0697, Val Loss: 0.0720\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "BiGRU = BiGRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(BiGRU, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5de64cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it hasasasasasasasasasasas'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('it has', 20, data.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f32d46",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7e09cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "\n",
    "class MTFraEng(DataModule):\n",
    "    def _download(self):\n",
    "        with open('Fra Eng Bilingual Sentence Pairs/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "               for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "\n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            if max_examples and i > max_examples: break\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                # Skip empty tokens\n",
    "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "        return src, tgt\n",
    "\n",
    "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "        \"\"\"Defined in :numref:`sec_machine_translation`\"\"\"\n",
    "        super(MTFraEng, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "            self._download())\n",
    "\n",
    "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            pad_or_trim = lambda seq, t: (\n",
    "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "            if is_tgt:\n",
    "                sentences = [['<bos>'] + s for s in sentences]\n",
    "            if vocab is None:\n",
    "                vocab = Vocab(sentences, min_freq=2)\n",
    "            array = torch.tensor([vocab[s] for s in sentences])\n",
    "            valid_len = reduce_sum(\n",
    "                astype(array != vocab['<pad>'], torch.int32), 1)\n",
    "            return array, vocab, valid_len\n",
    "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                                  self.num_train + self.num_val)\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),\n",
    "                src_vocab, tgt_vocab)\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
    "        idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "    def build(self, src_sentences, tgt_sentences):\n",
    "        \"\"\"Defined in :numref:`subsec_loading-seq-fixed-len`\"\"\"\n",
    "        raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "            src_sentences, tgt_sentences)])\n",
    "        arrays, _, _ = self._build_arrays(\n",
    "            raw_text, self.src_vocab, self.tgt_vocab)\n",
    "        return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17aae640",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = \u001b[43mMTFraEng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m src, tgt, src_valid_len, label = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(data.train_dataloader()))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33msource:\u001b[39m\u001b[33m'\u001b[39m, src.type(torch.int32))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mMTFraEng.__init__\u001b[39m\u001b[34m(self, batch_size, num_steps, num_train, num_val)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.num_train = num_train\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.num_val = num_val\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28mself\u001b[39m.arrays, \u001b[38;5;28mself\u001b[39m.src_vocab, \u001b[38;5;28mself\u001b[39m.tgt_vocab = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mMTFraEng._build_arrays\u001b[39m\u001b[34m(self, raw_text, src_vocab, tgt_vocab)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array, vocab, valid_len\n\u001b[32m     55\u001b[39m src, tgt = \u001b[38;5;28mself\u001b[39m._tokenize(\u001b[38;5;28mself\u001b[39m._preprocess(raw_text),\n\u001b[32m     56\u001b[39m                           \u001b[38;5;28mself\u001b[39m.num_train + \u001b[38;5;28mself\u001b[39m.num_val)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m src_array, src_vocab, src_valid_len = \u001b[43m_build_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ((src_array, tgt_array[:,:-\u001b[32m1\u001b[39m], src_valid_len, tgt_array[:,\u001b[32m1\u001b[39m:]),\n\u001b[32m     60\u001b[39m         src_vocab, tgt_vocab)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mMTFraEng._build_arrays.<locals>._build_array\u001b[39m\u001b[34m(sentences, vocab, is_tgt)\u001b[39m\n\u001b[32m     50\u001b[39m     vocab = Vocab(sentences, min_freq=\u001b[32m2\u001b[39m)\n\u001b[32m     51\u001b[39m array = torch.tensor([vocab[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m valid_len = \u001b[43mreduce_sum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m<pad>\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m array, vocab, valid_len\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x, *args, **kwargs)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m reduce_sum = \u001b[38;5;28;01mlambda\u001b[39;00m x, *args, **kwargs: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m astype = \u001b[38;5;28;01mlambda\u001b[39;00m x, *args, **kwargs: x.type(*args, **kwargs)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMTFraEng\u001b[39;00m(DataModule):\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "data = MTFraEng(batch_size=3)\n",
    "src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\n",
    "print('source:', src.type(torch.int32))\n",
    "print('decoder input:', tgt.type(torch.int32))\n",
    "print('source len excluding pad:', src_valid_len.type(torch.int32))\n",
    "print('label:', label.type(torch.int32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
