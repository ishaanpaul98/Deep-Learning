{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ecb66b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025e9920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import re\n",
    "import collections\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e6fae",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d43cac",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae04e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "text = \"\"\n",
    "full_text = \"\"\n",
    "with open('The Time Machine - Sample.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "    full_text = preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d4f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab: \n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']\n",
    "    \n",
    "def tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "def build(raw_text, vocab=None):\n",
    "    tokens = tokenize(preprocess(raw_text))\n",
    "    if vocab is None: vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for token in tokens]\n",
    "    return corpus, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c77930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageData(nn.Module):\n",
    "    def _download(self):\n",
    "        fname = \"The Time Machine - Sample.txt\"\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        return re.sub('[^A-Za-z]+', ' ', text).lower()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        return list(text)\n",
    "\n",
    "    def build(self, raw_text, vocab=None):\n",
    "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
    "        tokens = self._tokenize(self._preprocess(raw_text))\n",
    "        if vocab is None: vocab = Vocab(tokens)\n",
    "        corpus = [vocab[token] for token in tokens]\n",
    "        return corpus, vocab\n",
    "\n",
    "    def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
    "        \"\"\"Defined in :numref:`sec_language-model`\"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        corpus, self.vocab = self.build(self._download())\n",
    "        array = torch.tensor([corpus[i:i+num_steps+1]\n",
    "                            for i in range(len(corpus)-num_steps)])\n",
    "        self.X, self.Y = array[:,:-1], array[:,1:]\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        \"\"\"Defined in :numref:`subsec_partitioning-seqs`\"\"\"\n",
    "        idx = slice(0, self.num_train) if train else slice(\n",
    "            self.num_train, self.num_train + self.num_val)\n",
    "        return self.get_tensorloader([self.X, self.Y], train, idx)\n",
    "    \n",
    "    def get_tensorloader(self, tensors, train, idx_slice):\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            tensors[0][idx_slice], tensors[1][idx_slice]\n",
    "        )\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=train\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fe75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(nn.Module):\n",
    "    \"\"\"The RNN model implemented from scratch.\n",
    "\n",
    "    Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "        self.W_xh = nn.Parameter(\n",
    "            torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.W_hh = nn.Parameter(\n",
    "            torch.randn(num_hiddens, num_hiddens) * sigma)\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n",
    "\n",
    "    def forward(self, inputs, state=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        if state is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                              device=inputs.device)\n",
    "        else:\n",
    "            state, = state\n",
    "        outputs = []\n",
    "        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n",
    "            state = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                             torch.matmul(state, self.W_hh) + self.b_h)\n",
    "            outputs.append(state)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72caae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLMScratch(nn.Module): \n",
    "    \"\"\"The RNN-based language model implemented from scratch.\"\"\"\n",
    "    def __init__(self, rnn, vocab_size, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.rnn = rnn\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lr = lr\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        self.W_hq = nn.Parameter(\n",
    "            torch.randn(\n",
    "                self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
    "        self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('ppl', torch.exp(l), train=False)\n",
    "    \n",
    "    def one_hot(self, X):\n",
    "        # Output shape: (num_steps, batch_size, vocab_size)\n",
    "        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "    \n",
    "    def output_layer(self, rnn_outputs):\n",
    "        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]\n",
    "        return torch.stack(outputs, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def clip_gradients(self, grad_clip_val, model):\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "        if norm > grad_clip_val:\n",
    "            for param in params:\n",
    "                param.grad[:] *= grad_clip_val / norm\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        embs = self.one_hot(X)\n",
    "        rnn_outputs, _ = self.rnn(embs, state)\n",
    "        return self.output_layer(rnn_outputs)\n",
    "\n",
    "    def predict(self, prefix, num_preds, vocab, device=None):\n",
    "        \"\"\"Defined in :numref:`sec_rnn-scratch`\"\"\"\n",
    "        state, outputs = None, [vocab[prefix[0]]]\n",
    "        for i in range(len(prefix) + num_preds - 1):\n",
    "            X = torch.tensor([[outputs[-1]]], device=device)\n",
    "            embs = self.one_hot(X)\n",
    "            rnn_outputs, state = self.rnn(embs, state)\n",
    "            if i < len(prefix) - 1:  # Warm-up period\n",
    "                outputs.append(vocab[prefix[i + 1]])\n",
    "            else:  # Predict num_preds steps\n",
    "                Y = self.output_layer(rnn_outputs)\n",
    "                outputs.append(int(torch.reshape(torch.argmax(Y, axis=2), (1,))))\n",
    "        return ''.join([vocab.idx_to_token[i] for i in outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e37cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):  #@save\n",
    "    \"\"\"The RNN model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        # Initialize the RNN layer \n",
    "        self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        return self.rnn(inputs, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a64cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(RNNLMScratch):  #@save\n",
    "    \"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\n",
    "    def init_params(self):\n",
    "        self.linear = nn.LazyLinear(self.vocab_size)\n",
    "\n",
    "    def output_layer(self, hiddens):\n",
    "        return self.linear(hiddens).swapaxes(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53165170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, criterion, optimizer, num_epochs=100, grad_clip=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, Y in data.get_dataloader(train=True):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[-1]), Y.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data.get_dataloader(train=True))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in data.get_dataloader(train=False):\n",
    "                X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
    "                val_outputs = model(X_val)\n",
    "                v_loss = criterion(val_outputs.reshape(-1, val_outputs.shape[-1]), Y_val.reshape(-1))\n",
    "                val_loss += v_loss.item()\n",
    "        avg_val_loss = val_loss / len(data.get_dataloader(train=False))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd8b14",
   "metadata": {},
   "source": [
    "def train(model, data, criterion, optimizer, num_epochs=100, grad_clip=1):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, Y in data.get_dataloader(train=True):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[-1]), Y.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(data.get_dataloader(train=True))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, Y_val in data.get_dataloader(train=False):\n",
    "                val_outputs = model(X_val)\n",
    "                v_loss = criterion(val_outputs.reshape(-1, val_outputs.shape[-1]), Y_val.reshape(-1))\n",
    "                val_loss += v_loss.item()\n",
    "        avg_val_loss = val_loss / len(data.get_dataloader(train=False))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff732286",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f7e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens  \n",
    "        self.sigma = sigma\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xi, self.W_hi, self.b_i = triple()  # Input gate\n",
    "        self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate\n",
    "        self.W_xo, self.W_ho, self.b_o = triple()  # Output gate\n",
    "        self.W_xc, self.W_hc, self.b_c = triple()  # Input node\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        if H_C is None:\n",
    "            # Initial state with shape: (batch_size, num_hiddens)\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                        device=inputs.device)\n",
    "            C = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                        device=inputs.device)\n",
    "        else:\n",
    "            H, C = H_C\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            I = torch.sigmoid(torch.matmul(X, self.W_xi) +\n",
    "                            torch.matmul(H, self.W_hi) + self.b_i)\n",
    "            F = torch.sigmoid(torch.matmul(X, self.W_xf) +\n",
    "                            torch.matmul(H, self.W_hf) + self.b_f)\n",
    "            O = torch.sigmoid(torch.matmul(X, self.W_xo) +\n",
    "                            torch.matmul(H, self.W_ho) + self.b_o)\n",
    "            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +\n",
    "                            torch.matmul(H, self.W_hc) + self.b_c)\n",
    "            C = F * C + I * C_tilde\n",
    "            H = O * torch.tanh(C)\n",
    "            outputs.append(H)\n",
    "        return outputs, (H, C)\n",
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e335395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.7164, Val Loss: 2.6669\n",
      "Epoch 20, Train Loss: 2.3676, Val Loss: 2.3153\n",
      "Epoch 30, Train Loss: 2.1864, Val Loss: 2.2178\n",
      "Epoch 40, Train Loss: 2.0686, Val Loss: 2.1159\n",
      "Epoch 50, Train Loss: 1.9451, Val Loss: 2.0425\n",
      "Epoch 60, Train Loss: 1.8471, Val Loss: 1.9886\n",
      "Epoch 70, Train Loss: 1.7663, Val Loss: 1.9761\n",
      "Epoch 80, Train Loss: 1.7000, Val Loss: 1.9568\n",
      "Epoch 90, Train Loss: 1.6386, Val Loss: 1.9564\n",
      "Epoch 100, Train Loss: 1.5799, Val Loss: 1.9633\n",
      "Epoch 110, Train Loss: 1.5256, Val Loss: 1.9617\n",
      "Epoch 120, Train Loss: 1.4861, Val Loss: 1.9840\n",
      "Epoch 130, Train Loss: 1.4582, Val Loss: 2.0016\n",
      "Epoch 140, Train Loss: 1.4220, Val Loss: 2.0171\n",
      "Epoch 150, Train Loss: 1.4072, Val Loss: 2.0085\n",
      "Epoch 160, Train Loss: 1.3780, Val Loss: 2.0310\n",
      "Epoch 170, Train Loss: 1.3462, Val Loss: 2.0336\n",
      "Epoch 180, Train Loss: 1.3206, Val Loss: 2.0814\n",
      "Epoch 190, Train Loss: 1.3132, Val Loss: 2.0926\n",
      "Epoch 200, Train Loss: 1.2892, Val Loss: 2.0804\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e6415d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has of the time travell'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c661e30",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7637454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.rnn = nn.LSTM(num_inputs, num_hiddens)\n",
    "\n",
    "    def forward(self, inputs, H_C=None):\n",
    "        return self.rnn(inputs, H_C)\n",
    "\n",
    "lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d6c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.3893, Val Loss: 2.3807\n",
      "Epoch 20, Train Loss: 2.1294, Val Loss: 2.1925\n",
      "Epoch 30, Train Loss: 1.9624, Val Loss: 2.0632\n",
      "Epoch 40, Train Loss: 1.8592, Val Loss: 2.0350\n",
      "Epoch 50, Train Loss: 1.7515, Val Loss: 1.9708\n",
      "Epoch 60, Train Loss: 1.6922, Val Loss: 1.9643\n",
      "Epoch 70, Train Loss: 1.6352, Val Loss: 1.9695\n",
      "Epoch 80, Train Loss: 1.5637, Val Loss: 1.9927\n",
      "Epoch 90, Train Loss: 1.5226, Val Loss: 1.9670\n",
      "Epoch 100, Train Loss: 1.4815, Val Loss: 2.0110\n",
      "Epoch 110, Train Loss: 1.4593, Val Loss: 2.0170\n",
      "Epoch 120, Train Loss: 1.4348, Val Loss: 2.0462\n",
      "Epoch 130, Train Loss: 1.4013, Val Loss: 2.0554\n",
      "Epoch 140, Train Loss: 1.3745, Val Loss: 2.0369\n",
      "Epoch 150, Train Loss: 1.3465, Val Loss: 2.0517\n",
      "Epoch 160, Train Loss: 1.3358, Val Loss: 2.0900\n",
      "Epoch 170, Train Loss: 1.2994, Val Loss: 2.1052\n",
      "Epoch 180, Train Loss: 1.2951, Val Loss: 2.1229\n",
      "Epoch 190, Train Loss: 1.2816, Val Loss: 2.1243\n",
      "Epoch 200, Train Loss: 1.2655, Val Loss: 2.1604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'it has a moment another a '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "model = RNNLM(lstm, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)\n",
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394bef5",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81feb5",
   "metadata": {},
   "source": [
    "## Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daeb4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.sigma = sigma\n",
    "\n",
    "        init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)\n",
    "        triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                          init_weight(num_hiddens, num_hiddens),\n",
    "                          nn.Parameter(torch.zeros(num_hiddens)))\n",
    "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state\n",
    "\n",
    "    def forward(self, inputs, H=None):\n",
    "        if H is None:\n",
    "            H = torch.zeros((inputs.shape[1], self.num_hiddens), device=inputs.device)\n",
    "        outputs = []\n",
    "        for X in inputs:\n",
    "            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +\n",
    "                              torch.matmul(H, self.W_hz) + self.b_z)\n",
    "            R = torch.sigmoid(torch.matmul(X, self.W_xr) +\n",
    "                              torch.matmul(H, self.W_hr) + self.b_r)\n",
    "            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +\n",
    "                                 torch.matmul(R * H, self.W_hh) + self.b_h)\n",
    "            H = Z * H + (1 - Z) * H_tilde\n",
    "            outputs.append(H)\n",
    "        outputs = torch.stack(outputs)  # Stack list into tensor\n",
    "        return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5fdc98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.2921, Val Loss: 2.2610\n",
      "Epoch 20, Train Loss: 2.0540, Val Loss: 2.1085\n",
      "Epoch 30, Train Loss: 1.9041, Val Loss: 2.0376\n",
      "Epoch 40, Train Loss: 1.7825, Val Loss: 1.9793\n",
      "Epoch 50, Train Loss: 1.6834, Val Loss: 1.9564\n",
      "Epoch 60, Train Loss: 1.5717, Val Loss: 1.9509\n",
      "Epoch 70, Train Loss: 1.5304, Val Loss: 2.0264\n",
      "Epoch 80, Train Loss: 1.4759, Val Loss: 2.0018\n",
      "Epoch 90, Train Loss: 1.4211, Val Loss: 2.0265\n",
      "Epoch 100, Train Loss: 1.4012, Val Loss: 2.0878\n",
      "Epoch 110, Train Loss: 1.3970, Val Loss: 2.0825\n",
      "Epoch 120, Train Loss: 1.3642, Val Loss: 2.0705\n",
      "Epoch 130, Train Loss: 1.3380, Val Loss: 2.0991\n",
      "Epoch 140, Train Loss: 1.3145, Val Loss: 2.1121\n",
      "Epoch 150, Train Loss: 1.3068, Val Loss: 2.1344\n",
      "Epoch 160, Train Loss: 1.3072, Val Loss: 2.1521\n",
      "Epoch 170, Train Loss: 1.2845, Val Loss: 2.1829\n",
      "Epoch 180, Train Loss: 1.2893, Val Loss: 2.1811\n",
      "Epoch 190, Train Loss: 1.2634, Val Loss: 2.1888\n",
      "Epoch 200, Train Loss: 1.2666, Val Loss: 2.1820\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gruscratch = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(gruscratch, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "820896c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has all have a recentio'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687053c5",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a7dd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c3da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.2388, Val Loss: 2.2328\n",
      "Epoch 20, Train Loss: 2.0230, Val Loss: 2.0880\n",
      "Epoch 30, Train Loss: 1.8420, Val Loss: 1.9910\n",
      "Epoch 40, Train Loss: 1.7375, Val Loss: 1.9595\n",
      "Epoch 50, Train Loss: 1.6353, Val Loss: 1.9155\n",
      "Epoch 60, Train Loss: 1.5533, Val Loss: 1.9177\n",
      "Epoch 70, Train Loss: 1.5008, Val Loss: 1.9366\n",
      "Epoch 80, Train Loss: 1.4671, Val Loss: 1.9608\n",
      "Epoch 90, Train Loss: 1.4233, Val Loss: 1.9787\n",
      "Epoch 100, Train Loss: 1.4138, Val Loss: 2.0101\n",
      "Epoch 110, Train Loss: 1.3929, Val Loss: 2.0419\n",
      "Epoch 120, Train Loss: 1.3887, Val Loss: 2.0872\n",
      "Epoch 130, Train Loss: 1.3722, Val Loss: 2.0425\n",
      "Epoch 140, Train Loss: 1.3333, Val Loss: 2.1189\n",
      "Epoch 150, Train Loss: 1.3217, Val Loss: 2.0908\n",
      "Epoch 160, Train Loss: 1.3113, Val Loss: 2.0896\n",
      "Epoch 170, Train Loss: 1.3023, Val Loss: 2.1237\n",
      "Epoch 180, Train Loss: 1.2957, Val Loss: 2.1437\n",
      "Epoch 190, Train Loss: 1.2814, Val Loss: 2.0928\n",
      "Epoch 200, Train Loss: 1.2770, Val Loss: 2.1259\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "model = RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d90ade89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has experimental getter'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62242edc",
   "metadata": {},
   "source": [
    "# Deep Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c94a80",
   "metadata": {},
   "source": [
    "# Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "160d442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNNScratch(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.sigma = sigma\n",
    "        self.rnns = nn.Sequential(*[RNNScratch(\n",
    "            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)\n",
    "                                    for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, inputs, Hs=None):\n",
    "        outputs = inputs\n",
    "        if Hs is None: Hs = [None] * self.num_layers\n",
    "        for i in range(self.num_layers):\n",
    "            outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n",
    "            outputs = torch.stack(outputs, 0)\n",
    "        return outputs, Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51f1dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.8409, Val Loss: 2.8241\n",
      "Epoch 20, Train Loss: 2.8343, Val Loss: 2.8146\n",
      "Epoch 30, Train Loss: 2.5086, Val Loss: 2.4841\n",
      "Epoch 40, Train Loss: 2.2571, Val Loss: 2.2452\n",
      "Epoch 50, Train Loss: 2.1329, Val Loss: 2.2005\n",
      "Epoch 60, Train Loss: 1.9893, Val Loss: 2.0914\n",
      "Epoch 70, Train Loss: 1.9039, Val Loss: 2.0356\n",
      "Epoch 80, Train Loss: 1.8207, Val Loss: 2.0229\n",
      "Epoch 90, Train Loss: 1.7252, Val Loss: 2.0115\n",
      "Epoch 100, Train Loss: 1.6746, Val Loss: 2.0035\n",
      "Epoch 110, Train Loss: 1.6355, Val Loss: 2.0068\n",
      "Epoch 120, Train Loss: 1.5823, Val Loss: 2.0303\n",
      "Epoch 130, Train Loss: 1.5441, Val Loss: 2.0137\n",
      "Epoch 140, Train Loss: 1.5182, Val Loss: 2.0255\n",
      "Epoch 150, Train Loss: 1.5154, Val Loss: 2.0200\n",
      "Epoch 160, Train Loss: 1.4800, Val Loss: 2.0565\n",
      "Epoch 170, Train Loss: 1.4756, Val Loss: 2.0670\n",
      "Epoch 180, Train Loss: 1.4749, Val Loss: 2.0468\n",
      "Epoch 190, Train Loss: 1.4484, Val Loss: 2.0729\n",
      "Epoch 200, Train Loss: 1.4666, Val Loss: 2.0859\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n",
    "                              num_hiddens=32, num_layers=2)\n",
    "model = RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ad7e27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has expect a sigs and a'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a4aa2",
   "metadata": {},
   "source": [
    "# Concise Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "442eba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\n",
    "        nn.Module.__init__(self)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\n",
    "                          dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aabdd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 2.6632, Val Loss: 2.6238\n",
      "Epoch 20, Train Loss: 2.2923, Val Loss: 2.2718\n",
      "Epoch 30, Train Loss: 2.1271, Val Loss: 2.1554\n",
      "Epoch 40, Train Loss: 1.9892, Val Loss: 2.0818\n",
      "Epoch 50, Train Loss: 1.8747, Val Loss: 1.9976\n",
      "Epoch 60, Train Loss: 1.7470, Val Loss: 1.9572\n",
      "Epoch 70, Train Loss: 1.6434, Val Loss: 1.9385\n",
      "Epoch 80, Train Loss: 1.5625, Val Loss: 1.9301\n",
      "Epoch 90, Train Loss: 1.4876, Val Loss: 1.9630\n",
      "Epoch 100, Train Loss: 1.4128, Val Loss: 1.9737\n",
      "Epoch 110, Train Loss: 1.3538, Val Loss: 1.9884\n",
      "Epoch 120, Train Loss: 1.2944, Val Loss: 2.0166\n",
      "Epoch 130, Train Loss: 1.2428, Val Loss: 2.0621\n",
      "Epoch 140, Train Loss: 1.1949, Val Loss: 2.0937\n",
      "Epoch 150, Train Loss: 1.1767, Val Loss: 2.1665\n",
      "Epoch 160, Train Loss: 1.1318, Val Loss: 2.1602\n",
      "Epoch 170, Train Loss: 1.0935, Val Loss: 2.2200\n",
      "Epoch 180, Train Loss: 1.0633, Val Loss: 2.2401\n",
      "Epoch 190, Train Loss: 1.0553, Val Loss: 2.2827\n",
      "Epoch 200, Train Loss: 1.0355, Val Loss: 2.3040\n"
     ]
    }
   ],
   "source": [
    "data = LanguageData(batch_size=1024, num_steps=32)\n",
    "gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\n",
    "model = RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "train(model, data, nn.CrossEntropyLoss(), torch.optim.SGD(model.parameters(), lr=model.lr), num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a73e34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it has it there is howessi'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "model.predict('it has', 20, data.vocab, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
