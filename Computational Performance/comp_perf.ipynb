{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05633dd1",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dc300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba107f",
   "metadata": {},
   "source": [
    "# Symbolic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84088284",
   "metadata": {},
   "source": [
    "A simulation of a symbolic program (Since we are still using the Python interpreter here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ae52d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def fancy_func(a, b, c, d):\n",
      "    e = add(a, b)\n",
      "    f = add(c, d)\n",
      "    g = add(e, f)\n",
      "    return g\n",
      "print(fancy_func(1, 2, 3, 4))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
    "\n",
    "prog = evoke_()\n",
    "print(prog)\n",
    "y = compile(prog, '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83e851",
   "metadata": {},
   "source": [
    "# Hybridization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d92b390",
   "metadata": {},
   "source": [
    "Hybridization takes advantage of the imperative programming approach which makes the code very easy to read and debug and the fast execution and portability of symbolic programs that can avoid the bottleneck of the python interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a6771",
   "metadata": {},
   "source": [
    "## Hybridizing the Sequential Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb814402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0480, -0.2093]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Factory for networks\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2))\n",
    "    return net\n",
    "\n",
    "x = torch.randn(size=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a432b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0480, -0.2093]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.jit.script(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a80801",
   "metadata": {},
   "source": [
    "### Benchmarking the performance improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cefba3",
   "metadata": {},
   "source": [
    "While the compilation and execution seems identical to the earlier network, we can benchmark the performance to show the improvement made with hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c0eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Recording multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6dd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark:\n",
    "    \"\"\"For measuring running time.\"\"\"\n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.timer = Timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(f'{self.description}: {self.timer.stop():.4f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca13608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without torchscript: 0.0760 sec\n",
      "With torchscript: 0.0700 sec\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "with Benchmark('Without torchscript'):\n",
    "    for i in range(1000): net(x)\n",
    "\n",
    "net = torch.jit.script(net)\n",
    "with Benchmark('With torchscript'):\n",
    "    for i in range(1000): net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06899a9c",
   "metadata": {},
   "source": [
    "Here, we can see the difference in performance. As we create more complicated networks, the performance difference will matter more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f247af6",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad47d7",
   "metadata": {},
   "source": [
    "We can serialize (save) the model and parameters on the disk to use in a front-end agnostic manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3675a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('my_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ce656",
   "metadata": {},
   "source": [
    "# Asynchronous Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257bc83b",
   "metadata": {},
   "source": [
    "## Asynchrony via backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274ef74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "numpy: 0.5140 sec\n",
      "torch: 0.0010 sec\n"
     ]
    }
   ],
   "source": [
    "# Warmup for GPU computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "a = torch.randn(size=(1000, 1000), device=device)\n",
    "b = torch.mm(a, a)\n",
    "\n",
    "with Benchmark('numpy'):\n",
    "    for _ in range(10):\n",
    "        a = np.random.normal(size=(1000, 1000))\n",
    "        b = np.dot(a, a)\n",
    "\n",
    "with Benchmark('torch'):\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3442214",
   "metadata": {},
   "source": [
    "Clearly operations undertaken using numpy (CPU) are slower than operations that leverage the GPU like with PyTorch (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3ab80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0.0020 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark():\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f67a9c",
   "metadata": {},
   "source": [
    "It's also clear that the advantage is not just because of the GPU. The PyTorch backend (written in C++) processes variables asynchonously while the front end (Python notebook) waits for the operations to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e639d688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1, 2), device=device)\n",
    "y = torch.ones((1, 2), device=device)\n",
    "z = x * y + 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dedc1e",
   "metadata": {},
   "source": [
    "Behind the scenes, the front end (Python in this instance) will execute the first 3 statements and will only wait for the backend (C++) before outputting the value of z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59758f4",
   "metadata": {},
   "source": [
    "## Barriers and Blockers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3924fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch conversion: 0.0020 sec\n",
      "numpy conversion: 0.0010 sec\n",
      "scalar conversion: 0.0200 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('torch conversion'):\n",
    "    b = torch.mm(a, a)\n",
    "    b = b.cpu()\n",
    "    \n",
    "with Benchmark('numpy conversion'):\n",
    "    b = torch.mm(a, a)\n",
    "    b = b.cpu().numpy()\n",
    "\n",
    "with Benchmark('scalar conversion'):\n",
    "    b = torch.mm(a, a)\n",
    "    b = b.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eee37d",
   "metadata": {},
   "source": [
    ".numpy() and .item() conversion doesn't take advantage of asynchrony due to the implementation of numpy. Thus, these actions can slow down a program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e846c",
   "metadata": {},
   "source": [
    "# Automatic Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e3279",
   "metadata": {},
   "source": [
    "## Parallel Computation on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5147ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Device 0: NVIDIA GeForce RTX 5070\n",
      "Multiple GPUs: False\n"
     ]
    }
   ],
   "source": [
    "multiple_gpus = False\n",
    "device_count = torch.cuda.device_count()\n",
    "devices = []\n",
    "if device_count == 2:\n",
    "    multiple_gpus = True\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    devices.append(torch.cuda.get_device_name(i))\n",
    "print(f\"Multiple GPUs: {multiple_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5793057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need at least 2 GPUs to run this example\n"
     ]
    }
   ],
   "source": [
    "def run(x):\n",
    "    return [x.mm(x) for _ in range(50)]\n",
    "if multiple_gpus:\n",
    "    x_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])\n",
    "    x_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])\n",
    "else:\n",
    "    print(f\"You need at least 2 GPUs to run this example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7154465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need at least 2 GPUs to run this example\n"
     ]
    }
   ],
   "source": [
    "if multiple_gpus:    \n",
    "    run(x_gpu1)\n",
    "    run(x_gpu2)  # Warm-up all devices\n",
    "    torch.cuda.synchronize(devices[0])\n",
    "    torch.cuda.synchronize(devices[1])\n",
    "\n",
    "    with Benchmark('GPU1 time'):\n",
    "        run(x_gpu1)\n",
    "        torch.cuda.synchronize(devices[0])\n",
    "\n",
    "    with Benchmark('GPU2 time'):\n",
    "        run(x_gpu2)\n",
    "        torch.cuda.synchronize(devices[1])\n",
    "else:\n",
    "    print(f\"You need at least 2 GPUs to run this example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bddd4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need at least 2 GPUs to run this example\n"
     ]
    }
   ],
   "source": [
    "if multiple_gpus:\n",
    "    with Benchmark('GPU1 & GPU2'):\n",
    "        run(x_gpu1)\n",
    "        run(x_gpu2)\n",
    "        torch.cuda.synchronize()\n",
    "else:\n",
    "    print(f\"You need at least 2 GPUs to run this example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1343f4",
   "metadata": {},
   "source": [
    "## Parallel computation and communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d079d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU1: 0.6140 sec\n",
      "Copy to CPU: 1.2468 sec\n"
     ]
    }
   ],
   "source": [
    "def copy_to_cpu(x, non_blocking=False):\n",
    "    return [y.to('cpu', non_blocking=non_blocking) for y in x]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'none')\n",
    "x_gpu1 = torch.rand(size=(4000, 4000), device=device)\n",
    "run(x_gpu1)  # Warm-up\n",
    "with Benchmark('Run on GPU1'):\n",
    "    y = run(x_gpu1)\n",
    "    torch.cuda.synchronize()\n",
    "with Benchmark('Copy to CPU'):\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee4eee",
   "metadata": {},
   "source": [
    "Clearly copying the tensor to CPU is expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5526d663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU1 and copy to CPU: 0.7340 sec\n"
     ]
    }
   ],
   "source": [
    "with Benchmark('Run on GPU1 and copy to CPU'):\n",
    "    y = run(x_gpu1)\n",
    "    y_cpu = copy_to_cpu(y, True)\n",
    "    torch.cuda.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
